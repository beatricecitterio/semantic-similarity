{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Track 3 - Bonus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE DATA\n",
    "dev_df = pd.read_csv('dev_responses.csv')\n",
    "train_df = pd.read_csv('train_responses.csv')\n",
    "\n",
    "## REMOVE INVALID RESPONSES\n",
    "train_df = train_df[\n",
    "    train_df['model_response'].astype(str).str.strip().replace(r'^\\W*$', '', regex=True) != ''\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2') # this has proved to be the best model for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(text):\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = np.vstack([get_vec(p) for p in train_df['user_prompt']])\n",
    "dev_vecs = np.vstack([get_vec(p) for p in dev_df['user_prompt']])\n",
    "\n",
    "train_vecs = normalize(train_vecs)\n",
    "dev_vecs = normalize(dev_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE BLEU SCORE\n",
    "smoother = SmoothingFunction().method3\n",
    "def evaluate_bleu(dev_df, retrieved_responses):\n",
    "    scores = []\n",
    "    for i in range(len(dev_df)):\n",
    "        ref = str(dev_df.loc[i, 'model_response']).split()\n",
    "        hyp = str(retrieved_responses[i]).split()\n",
    "        score = sentence_bleu([ref], hyp, weights=(0.5, 0.5, 0, 0), smoothing_function=smoother)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEAREST NEIGHBORS\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='brute', metric='cosine')\n",
    "nn.fit(train_vecs)\n",
    "_, idxs = nn.kneighbors(dev_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.1080\n"
     ]
    }
   ],
   "source": [
    "retrieved_responses = [train_df.iloc[i]['model_response'] for i in idxs.flatten()]\n",
    "\n",
    "bleu = evaluate_bleu(dev_df, retrieved_responses)\n",
    "print(f\"BLEU: {bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried also other models, which gave the following results:\n",
    "- all-MiniLM-L6-v2 gave BLEU 0.1024 \n",
    "- all-MiniLM-L12-v2 gave BLEU: 0.1048 \n",
    "- paraphrase-MiniLM-L6-v2 gave BLEU: 0.0967 \n",
    "- paraphrase-mpnet-base-v2 gave BLEU: 0.1025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Submission CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_track3_submission(train_df, dev_df, test_df, output_file='track_3_test.csv'):\n",
    "    combined_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "    combined_df = combined_df[\n",
    "        combined_df['model_response'].astype(str).str.strip().replace(r'^\\W*$', '', regex=True) != ''\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')  \n",
    "\n",
    "    combined_vecs = model.encode(combined_df['user_prompt'].tolist(), normalize_embeddings=True)\n",
    "    test_vecs = model.encode(test_df['user_prompt'].tolist(), normalize_embeddings=True)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=1, algorithm='brute', metric='cosine')\n",
    "    nn.fit(combined_vecs)\n",
    "    _, idxs = nn.kneighbors(test_vecs)\n",
    "\n",
    "    matched_ids = [combined_df.iloc[i]['conversation_id'] for i in idxs.flatten()]\n",
    "    result_df = pd.DataFrame({\n",
    "        'conversation_id': test_df['conversation_id'],\n",
    "        'response_id': matched_ids\n",
    "    })\n",
    "\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved Track 2 submission to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Track 2 submission to: track_3_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_prompts.csv')\n",
    "generate_track3_submission(train_df, dev_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
